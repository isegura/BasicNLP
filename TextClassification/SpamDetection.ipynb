{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/TextClassification/blob/master/SpamDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPkfUyg1EXpD",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification: spam detection\n",
        "\n",
        "\n",
        "\n",
        "This notebook is about text classification. Spam detection is a binary text classification problem because there is only two classes:  'spam' and 'ham' (non-spam).\n",
        "\n",
        "The dataset can be downloaded from https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRNgZ2MTEXpR",
        "colab_type": "text"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "First, we need to mountain the google drive folder.  To do this, please run the following cell and follow the instructions. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM0jAoJ4E6qX",
        "colab_type": "code",
        "outputId": "f93cbd06-0166-4154-a8a5-a2982fc97f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfhqodd6E8sp",
        "colab_type": "text"
      },
      "source": [
        "The file **SMSSpamCollection** contains the dataset. Each line represents a message. Each line starts with the label, followed by a tab and then the message text. \n",
        "\n",
        "To load the file, you can read line by line and extract the information using the method split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMQasmHkU5oE",
        "colab_type": "code",
        "outputId": "39a97b9f-4e10-4a0b-96e3-84d5753ffd59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sst_home='drive/My Drive/Colab Notebooks/'\n",
        "#modify this path \n",
        "path=sst_home+'TESI/3-TextClasification/data/SMSSpamCollection'\n",
        "\n",
        "messages=[]\n",
        "labels=[]\n",
        "f=open(path, \"r\")\n",
        "for line in f.readlines():\n",
        "  #print(line)\n",
        "  data=line.split('\\t')\n",
        "  labels.append(data[0])\n",
        "  messages.append(data[1])\n",
        "\n",
        "print(len(messages),len(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5574 5574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPh12hXcUrfD",
        "colab_type": "text"
      },
      "source": [
        "However, there are several Python libraries to load easily files whose lines contain fields separated by tab or commas.\n",
        "\n",
        "One of the most popular library for loading data structure is **Pandas**. The following cell shows how you can load the data using it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1sFe_s2EXpT",
        "colab_type": "code",
        "outputId": "d0e0069b-402d-4258-b872-9f9afd180610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import pandas\n",
        "import csv\n",
        "\n",
        "messages = pandas.read_csv(path, sep='\\t', quoting=csv.QUOTE_NONE, names=[\"label\", \"message\"])\n",
        "\n",
        "#shows the first messages\n",
        "print(messages.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPbdgdvEXpZ",
        "colab_type": "text"
      },
      "source": [
        "## Representing texts\n",
        "\n",
        "Each message in the dataset must be represented by a set of features. \n",
        "The most common approach for representing texts for the text classification is to use the bag-of-words models. In this model, each text is represented by a vector. Each element of the word is the frequency of each word of the vocabulary extracted from the dataset.\n",
        " \n",
        " ### Cleaning\n",
        " \n",
        "In order to reduce the lexical variability, we use stems instead of tokens. Moreover, we remove stop words, words with digits or special characters, and words with a lenght lower than 3. \n",
        "\n",
        "Moreover, we use the library **nltk** to obtain the stop words and the stems of the tokens. \n",
        "\n",
        "The **cleanText** functions takes a text as input and cleans it removing the stop words,  words with length lower than 3 and words containing numbers or spacial characters. The list of its stems is returned. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUCZiIEgEXpb",
        "colab_type": "code",
        "outputId": "b07a155a-fd4c-4c45-d3fe-a16c2a7bf826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "\n",
        "\n",
        "def cleanText(text):\n",
        "    text=str(text).lower()\n",
        "    \n",
        "    #tokeniza the text\n",
        "    tokens=word_tokenize(text)\n",
        "\n",
        "\n",
        "    \n",
        "    #remove the stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords_en]\n",
        "\n",
        "    \n",
        "    \n",
        "    #(4) obtain the stems\n",
        "    tokens = [PorterStemmer().stem(word) for word in tokens]\n",
        "\n",
        "    \n",
        "    #(5) finally, remove words with len <3 and words that contain numbers, puntuaction, ect\n",
        "    min_length = 3\n",
        "    p = re.compile('[a-zA-Z]+');\n",
        "    filtered_tokens=[]\n",
        "    for token in tokens:\n",
        "        if len(token)>=min_length and p.match(token):\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    return filtered_tokens\n",
        "\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXfpVg1HZ9Pv",
        "colab_type": "text"
      },
      "source": [
        "Let us to apply this function to the first texts of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUlCVXBfaBig",
        "colab_type": "code",
        "outputId": "30ca12cc-cfd9-4e65-95ea-f98fbae7e1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "\n",
        "#messages.message.head().apply(cleanText)\n",
        "aux=messages.message.head()\n",
        "for text in aux:\n",
        "  print(text)\n",
        "  print(cleanText(text))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "['jurong', 'point', 'crazy..', 'avail', 'bugi', 'great', 'world', 'buffet', 'cine', 'got', 'amor', 'wat']\n",
            "\n",
            "Ok lar... Joking wif u oni...\n",
            "['lar', 'joke', 'wif', 'oni']\n",
            "\n",
            "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "['free', 'entri', 'wkli', 'comp', 'win', 'cup', 'final', 'tkt', 'may', 'text', 'receiv', 'entri', 'question', 'std', 'txt', 'rate', 'appli']\n",
            "\n",
            "U dun say so early hor... U c already then say...\n",
            "['dun', 'say', 'earli', 'hor', 'alreadi', 'say']\n",
            "\n",
            "Nah I don't think he goes to usf, he lives around here though\n",
            "['nah', \"n't\", 'think', 'goe', 'usf', 'live', 'around', 'though']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo_56mlvEXpg",
        "colab_type": "text"
      },
      "source": [
        "### Vectorization\n",
        "\n",
        "The **CountVectorizer** class of the sklearn library (https://scikit-learn.org/) allows us to obtain the vocabulary of the dataset (the set of unique words). We can see that the size of the vocabulary is 6,928 unique words (stems).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dunOUYJXEXpi",
        "colab_type": "code",
        "outputId": "8dffd0e2-3d87-4019-ad2d-d48dbca30449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow = CountVectorizer(analyzer=cleanText).fit(messages['message'])\n",
        "#print(bow_transformer.vocabulary_)\n",
        "print(len(bow.vocabulary_))\n",
        "\n",
        "#we show the word with index 0 at the vocabulary\n",
        "print(bow.get_feature_names()[0])\n",
        "\n",
        "#we show the word with index 253 at the vocabulary\n",
        "print(bow.get_feature_names()[253])\n",
        "#we show the word with index 1291 at the vocabulary\n",
        "print(bow.get_feature_names()[1291])\n",
        "#we show the word with index 1813 at the vocabulary\n",
        "print(bow.get_feature_names()[1813])\n",
        "#we show the word with index 2375 at the vocabulary\n",
        "print(bow.get_feature_names()[2375])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6928\n",
            "a-ffection\n",
            "anymor\n",
            "cri\n",
            "enough\n",
            "gon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKIhwzmGEXpo",
        "colab_type": "text"
      },
      "source": [
        "Now, we can transform any message to a vector based on this vocabulary. To do this, we will use the method **transform**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OBZLM68EXpr",
        "colab_type": "code",
        "outputId": "98652e96-25ba-4d56-a80e-d89e55680a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "message=messages['message'][10]\n",
        "print(message)\n",
        "vector = bow.transform([message])\n",
        "print(vector.shape)\n",
        "print(vector)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
            "(1, 6928)\n",
            "  (0, 253)\t1\n",
            "  (0, 1291)\t1\n",
            "  (0, 1813)\t1\n",
            "  (0, 2375)\t1\n",
            "  (0, 2686)\t1\n",
            "  (0, 3868)\t1\n",
            "  (0, 5479)\t1\n",
            "  (0, 5694)\t1\n",
            "  (0, 5840)\t1\n",
            "  (0, 6070)\t1\n",
            "  (0, 6110)\t1\n",
            "  (0, 6513)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTPkpeN2EXp9",
        "colab_type": "text"
      },
      "source": [
        "Now, we can apply the model to the whole collection in order to tranform their messages to vectors. \n",
        "\n",
        "The property **nnz** allows us to obtain the number of non-zeros elements in the matrix. You can see that only the 0.11\\% of the elements in the matrix are different to 0. Therefore, the matrix data are very sparse. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qGR3nElEXp_",
        "colab_type": "code",
        "outputId": "518c01e5-33b6-439c-ec7a-6c76b638baae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "matrix_bow = bow.transform(messages['message'])\n",
        "print('sparse matrix shape:', matrix_bow.shape)\n",
        "print('total elements:',matrix_bow.shape[0] * matrix_bow.shape[1])\n",
        "print( 'number of non-zeros:', matrix_bow.nnz)\n",
        "print( 'sparsity: %.2f%%' % (100.0 * matrix_bow.nnz / (matrix_bow.shape[0] * matrix_bow.shape[1])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sparse matrix shape: (5574, 6928)\n",
            "total elements: 38616672\n",
            "number of non-zeros: 40774\n",
            "sparsity: 0.11%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfiIpDQeEXqF",
        "colab_type": "text"
      },
      "source": [
        "#### Dealing with the sparcity problem: TF-IDF model.\n",
        "\n",
        "To avoid that most elements in matrix are 0 (almost 99%), we can apply the TF-IDF model, where instead of using the frequency of each word, we take into account the frequency of the word in the whole collections of texts. In this way, we achive that those words very common in many texts, have a low weigth (actually, they are not discriminative words).\n",
        "\n",
        "TF-IDF for a term can be obtained with the following equation:\n",
        "\n",
        "${\\displaystyle \\mathrm {tfidf} (t,d,D)=\\mathrm {tf} (t,d)\\cdot \\mathrm {idf} (t,D)} =\n",
        "\\frac{f(t,d)}{max\\{f(w,d):w \\in d\\} } \\cdot  \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|} $\n",
        "\n",
        "\n",
        "where t is the word, d is the document and D the collection of texts.\n",
        "\n",
        "\n",
        "Fortunately, the sklearn library makes this task for us. You can use the **TfidfTransformer** class, which takes a matrix created with the bow approach, and transforms it to a new matrix based on TF-IDF. Moreover, the normalization L2 is applied. The norm of a vector is obtained dividing each element by the norm of the vector: \n",
        "\n",
        "$v = \\frac{v}{norm(v)}$\n",
        "\n",
        "where norm is usually L2 norm  (http://mathworld.wolfram.com/L2-Norm.html)\n",
        "\n",
        "<img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/348cef86ef91aa2d9a7151031a4fb80578090c4d'/>\n",
        "\n",
        "\n",
        "The following cells show ho to train a model from the bow matrix, and then, transform it to its tf-idf equivalent matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS5AQiBtEXqG",
        "colab_type": "code",
        "outputId": "49a19d71-d4c2-4f1d-a757-fe643fdd02e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#first, we train the transformer to tf-idf model\n",
        "tfidf_transformer = TfidfTransformer().fit(matrix_bow)\n",
        "print(tfidf_transformer.idf_[bow.vocabulary_['alreadi']])\n",
        "\n",
        "#now, we transform the bow matrix to a new matrix based on tf-idf\n",
        "tfidf_vectors = tfidf_transformer.transform(matrix_bow)\n",
        "print(tfidf_vectors.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.137411226596179\n",
            "(5574, 6928)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXmBtrQ-EXqL",
        "colab_type": "text"
      },
      "source": [
        "## Training \n",
        "\n",
        "Once we already know how to represent texts using the tf-idf model, you can train a classifier. In this tutorial, we will use the classifier **Naive Bayes**, which is implemented in the sklearn library. \n",
        "\n",
        "As first step, we must divide the dataset into training and test sets with the ratio 80-20. To do this, we use the method **train_test_split** of the package **model_selection**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b-Ely32EXqM",
        "colab_type": "code",
        "outputId": "b0852d79-0fa9-480f-a2c6-eba6191051b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, train_test_split \n",
        "\n",
        "msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'], test_size=0.2)\n",
        "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4459 1115 5574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtlMzXVEXqS",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to train a tf-idf model using the training dataset. Then, we will apply this tf-idf model to tranform the training and test sets. Remember that first you need to obtain the bow model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3TLq97pEXqU",
        "colab_type": "code",
        "outputId": "1619a720-629f-41e9-f5ce-8ca6e686fe0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bow = CountVectorizer(analyzer=cleanText).fit(msg_train)\n",
        "#transform the training set to bow model\n",
        "bow_train = bow.transform(msg_train)\n",
        "#transform the test set to bow model\n",
        "bow_test=bow.transform(msg_test)\n",
        "\n",
        "\n",
        "#learn the tf-idf model from the training bow\n",
        "tfidf_transformer = TfidfTransformer().fit(bow_train)\n",
        "#transform the training set to tf-idf model\n",
        "tfidf_train = tfidf_transformer.transform(bow_train)\n",
        "#transform the test set to tf-idf model\n",
        "tfidf_test = tfidf_transformer.transform(bow_test)\n",
        "\n",
        "print('matrices obtained')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matrices obtained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5F0VuYVEXqY",
        "colab_type": "text"
      },
      "source": [
        "Now, we can already train our classifier. You need to create an instance of the **MultinomialNB** class, which implements the **Naive Bayes** classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shHyYDEFEXqZ",
        "colab_type": "code",
        "outputId": "085f61a4-fa35-4156-a268-38bdcd7c71d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB()\n",
        "%time nb.fit(tfidf_train, label_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 19.2 ms, sys: 32 Âµs, total: 19.2 ms\n",
            "Wall time: 25.8 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH0Bqd0Yntjn",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation  \n",
        "\n",
        "Once the model has been trained, you can apply it on the test set in order to obtain the predictions. \n",
        "\n",
        "We can compare these predictions to the gold labels of the test set (label_test) in order to know the performance of the classifier. \n",
        "\n",
        "The metrics used to evaluate text classification systems are: \n",
        "\n",
        "- **Recall** refers to the ability of a model to find all the revelent cases within a dataset. Its equation is: $ recall = \\frac{TP}{TP + FN}$\n",
        "\n",
        "- **Precision** refers to the ability  of a  model to identify only the relevant data points. Its equation is: $precision= \\frac{TP}{TP + FP}$\n",
        "\n",
        "\n",
        "In some cases, we might know that we want to maximize either recall or precision at the expense of the other metric.  However, in many cases where we want to find an optimal blend of precision and recall. To do this, we can combine the two metrics using the **F1 score**, which is the harmonic mean of precision and recall taking both metrics into account in the following equation:\n",
        "$F1= 2*\\frac{precision*recall}{precision + recall}$\n",
        "\n",
        "\n",
        "Sklearn provides several functions which help us to measure the performance of the classifier. For example, the function **confusion_matrix** returns the confusion matrix. \n",
        "\n",
        "In binary classification, the count of true negatives is C$_0,_0$ , false negatives is C$_0,_1$  , true positives is C$_1,_1$ and false positives is  C$_1,_0$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL5rStV-nrJS",
        "colab_type": "code",
        "outputId": "42863a52-5261-4544-bfd2-d22cb1b87781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "predictions = nb.predict(tfidf_test)\n",
        "\n",
        "print( '(row=expected, col=predicted)')\n",
        "print( 'confusion matrix\\n', confusion_matrix(label_test, predictions))\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(label_test, predictions).ravel()\n",
        "print('tn, fp, fn, tp:',tn,fp,fn,tp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(row=expected, col=predicted)\n",
            "confusion matrix\n",
            " [[974   1]\n",
            " [ 47  93]]\n",
            "tn, fp, fn, tp: 974 1 47 93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiW4gpfran28",
        "colab_type": "text"
      },
      "source": [
        "Using the confusion matrix, we could calculate the scores for precision, recall and F1. However, **sklearn** already provides functions to obtains these scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg-BAxYUbDWL",
        "colab_type": "code",
        "outputId": "81d2527d-fb0c-4451-d7f3-715ed3331013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "print('accuracy', accuracy_score(label_test, predictions))\n",
        "print(classification_report(label_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.95695067264574\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      1.00      0.98       975\n",
            "        spam       0.99      0.66      0.79       140\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.97      0.83      0.89      1115\n",
            "weighted avg       0.96      0.96      0.95      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoitGNz_EXqe",
        "colab_type": "text"
      },
      "source": [
        "## Creating a pipeline\n",
        "\n",
        "Sklearn allows us to define a sequence of processes that will be excuted one after the other. \n",
        "\n",
        "Thus, we can create a pipeline where the first process is to create the bow mode, then the tf-idf model and finally we will apply a classifier (for example, naive bayes)\n",
        "\n",
        "The following cell shows how to create a pipeline that joins the processes created in the previous cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3ybSOHGEXqh",
        "colab_type": "code",
        "outputId": "7e70f28f-6634-48b1-c5d5-9dd28da0a087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer(analyzer=cleanText)),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
        "])\n",
        "\n",
        "pipeline.fit(msg_train,label_train)\n",
        "predictions = pipeline.predict(msg_test)\n",
        "\n",
        "print( classification_report(label_test, predictions))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      1.00      0.98       975\n",
            "        spam       0.99      0.66      0.79       140\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.97      0.83      0.89      1115\n",
            "weighted avg       0.96      0.96      0.95      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}